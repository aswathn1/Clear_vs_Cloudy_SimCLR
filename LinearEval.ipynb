{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sthalles/SimCLR/blob/master/feature_eval/mini_batch_logistic_regression_evaluator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUemQib7ZE4D"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOIJEui1ZziV"
   },
   "outputs": [],
   "source": [
    "def get_file_id_by_model(folder_name):\n",
    "  file_id = {'resnet-18_40-epochs': '1c4eVon0sUd-ChVhH6XMpF6nCngNJsAPk',\n",
    "             'resnet-18_80-epochs': '1L0yoeY9i2mzDcj69P4slTWb-cfr3PyoT',\n",
    "             'resnet-50_40-epochs': '1TZqBNTFCsO-mxAiR-zJeyupY-J2gA27Q',\n",
    "             'resnet-50_80-epochs': '1is1wkBRccHdhSKQnPUTQoaFkVNSaCb35',\n",
    "             'resnet-18_100-epochs':'1aZ12TITXnajZ6QWmS_SDm8Sp8gXNbeCQ'}\n",
    "  return file_id.get(folder_name, \"Model not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7YMxsvEZMrX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/radhakrishnan.39a/Documents/NN/SimCLR-2/runs/Apr25_15-32-48_cse-dcse101551d.coeit.osu.edu/ Model not found.\n"
     ]
    }
   ],
   "source": [
    "folder_name = '/home/radhakrishnan.39a/Documents/NN/SimCLR-2/runs/Apr25_15-32-48_cse-dcse101551d.coeit.osu.edu/'\n",
    "file_id = get_file_id_by_model(folder_name)\n",
    "print(folder_name, file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_nypQVEv-hn"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDfbL3w_Z0Od"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQMIryc6LjQd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-42da58e34c2b>:2: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(open(os.path.join(checkpoints_folder, \"config.yaml\"), \"r\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 512,\n",
       " 'epochs': 80,\n",
       " 'eval_every_n_epochs': 1,\n",
       " 'fine_tune_from': 'None',\n",
       " 'log_every_n_steps': 50,\n",
       " 'weight_decay': '10e-6',\n",
       " 'fp16_precision': False,\n",
       " 'model': {'out_dim': 256, 'base_model': 'resnet18'},\n",
       " 'dataset': {'data_path': '/home/radhakrishnan.39a/Documents/NN/Sentinel_Dataset',\n",
       "  's': 1,\n",
       "  'input_shape': '(64,64,3)',\n",
       "  'num_workers': 0,\n",
       "  'valid_size': 0.05},\n",
       " 'loss': {'temperature': 0.5, 'use_cosine_similarity': True}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints_folder = os.path.join(folder_name, 'checkpoints')\n",
    "config = yaml.load(open(os.path.join(checkpoints_folder, \"config.yaml\"), \"r\"))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder('/home/radhakrishnan.39a/Documents/NN/Sentinel_Dataset/train_10000/',transform=transforms.ToTensor())\n",
    "\n",
    "test_dataset = ImageFolder('/home/radhakrishnan.39a/Documents/NN/Sentinel_Dataset/test/',transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=100,\n",
    "                            num_workers=0, drop_last=False, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=500,\n",
    "                            num_workers=0, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a18lPD-tIle6"
   },
   "outputs": [],
   "source": [
    "def _load_resnet_model(checkpoints_folder):\n",
    "  # Load the neural net module\n",
    "  spec = importlib.util.spec_from_file_location(\"model\", os.path.join(checkpoints_folder, 'resnet_simclr.py'))\n",
    "  resnet_module = importlib.util.module_from_spec(spec)\n",
    "  spec.loader.exec_module(resnet_module)\n",
    "\n",
    "  model = resnet_module.ResNetSimCLR(**config['model'])\n",
    "  model.eval()\n",
    "\n",
    "  state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'), map_location=torch.device('cpu'))\n",
    "  model.load_state_dict(state_dict)\n",
    "  model = model.to(device)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nf4rDtWLjRE"
   },
   "source": [
    "## Logisitc Regression LinearEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jjSxmDnHNQz"
   },
   "outputs": [],
   "source": [
    "class ResNetFeatureExtractor(object):\n",
    "  def __init__(self, checkpoints_folder, train_loader, test_loader):\n",
    "    self.checkpoints_folder = checkpoints_folder\n",
    "    self.model = _load_resnet_model(checkpoints_folder)\n",
    "    self.train_loader = train_loader\n",
    "    self.test_loader = test_loader\n",
    "\n",
    "  def _inference(self, loader):\n",
    "    feature_vector = []\n",
    "    labels_vector = []\n",
    "    for batch_x, batch_y in loader:\n",
    "\n",
    "      batch_x = batch_x.to(device)\n",
    "      labels_vector.extend(batch_y)\n",
    "\n",
    "      features, _ = self.model(batch_x)\n",
    "      feature_vector.extend(features.cpu().detach().numpy())\n",
    "\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    labels_vector = np.array(labels_vector)\n",
    "\n",
    "    print(\"Features shape {}\".format(feature_vector.shape))\n",
    "    return feature_vector, labels_vector\n",
    "\n",
    "  def get_resnet_features(self):\n",
    "    train_loader, test_loader = self.train_loader, self.test_loader\n",
    "    X_train_feature, y_train = self._inference(train_loader)\n",
    "    X_test_feature, y_test = self._inference(test_loader)\n",
    "\n",
    "    return X_train_feature, y_train, X_test_feature, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kghx1govJq5_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor: resnet18\n"
     ]
    }
   ],
   "source": [
    "resnet_feature_extractor = ResNetFeatureExtractor('/home/radhakrishnan.39a/Documents/NN/SimCLR-2/runs/Apr25_15-32-48_cse-dcse101551d.coeit.osu.edu/checkpoints/', train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_JcznxVJ1Xj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape (10000, 512)\n",
      "Features shape (15240, 512)\n"
     ]
    }
   ],
   "source": [
    "X_train_feature, y_train, X_test_feature, y_test = resnet_feature_extractor.get_resnet_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oftbHXcdLjRM"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.model = nn.Linear(n_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ks73ePLtNWeV"
   },
   "outputs": [],
   "source": [
    "class LogiticRegressionEvaluator(object):\n",
    "  def __init__(self, n_features, n_classes):\n",
    "    self.log_regression = LogisticRegression(n_features, n_classes).to(device)\n",
    "    self.scaler = preprocessing.StandardScaler()\n",
    "\n",
    "  def _normalize_dataset(self, X_train, X_test):\n",
    "    print(\"Standard Scaling Normalizer\")\n",
    "    self.scaler.fit(X_train)\n",
    "    X_train = self.scaler.transform(X_train)\n",
    "    X_test = self.scaler.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "  @staticmethod\n",
    "  def _sample_weight_decay():\n",
    "    # We selected the l2 regularization parameter from a range of 45 logarithmically spaced values between 10âˆ’6 and 105\n",
    "    weight_decay = np.logspace(-6, 5, num=45, base=10.0)\n",
    "    weight_decay = np.random.choice(weight_decay)\n",
    "    print(\"Sampled weight decay:\", weight_decay)\n",
    "    return weight_decay\n",
    "\n",
    "  def eval(self, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      self.log_regression.eval()\n",
    "      for batch_x, batch_y in test_loader:\n",
    "          batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "          logits = self.log_regression(batch_x)\n",
    "\n",
    "          prediction = torch.argmax(logits, dim=1)\n",
    "          total += batch_y.size(0)\n",
    "          correct += (prediction == batch_y).sum().item()\n",
    "          for i in range(len(prediction.int())):\n",
    "#             if (prediction.int()[i] == labels.data.int()[i]):\n",
    "#                 acc_c+=1\n",
    "            if ((prediction[i] == 0) and (batch_y.data[i] == 0)):\n",
    "                tp +=1\n",
    "            elif ((prediction[i] == 0) and (batch_y.data[i] == 1)):\n",
    "                fp+=1\n",
    "                #fpl.append(fnames[i])\n",
    "            elif ((prediction[i] == 1) and (batch_y.data[i] == 1)):\n",
    "                tn+=1\n",
    "                #tnl.append(fnames[i])\n",
    "            elif ((prediction[i] == 1) and (batch_y.data[i] == 0)):\n",
    "                fn+=1\n",
    "                #fnl.append(fnames[i])\n",
    "        \n",
    "        \n",
    "\n",
    "      final_acc = 100 * correct / total\n",
    "      prec = tp/(tp+fp)\n",
    "      rec = tp/(tp+fn)\n",
    "      f1 = 2 * ((prec*rec)/(prec+rec))\n",
    "    \n",
    "      self.log_regression.train()\n",
    "      return final_acc, prec, rec, f1\n",
    "\n",
    "  def checkpoint_eval(self, ckp_fp, X_test, y_test):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    state_dict = torch.load(ckp_fp)\n",
    "    test = torch.utils.data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test).type(torch.long))\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=512, shuffle=False)\n",
    "    self.log_regression.load_state_dict(state_dict)\n",
    "    with torch.no_grad():\n",
    "      self.log_regression.eval()\n",
    "      for batch_x, batch_y in test_loader:\n",
    "          batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "          logits = self.log_regression(batch_x)\n",
    "\n",
    "          prediction = torch.argmax(logits, dim=1)\n",
    "          total += batch_y.size(0)\n",
    "          correct += (prediction == batch_y).sum().item()\n",
    "          for i in range(len(prediction.int())):\n",
    "#             if (prediction.int()[i] == labels.data.int()[i]):\n",
    "#                 acc_c+=1\n",
    "            if ((prediction[i] == 0) and (batch_y.data[i] == 0)):\n",
    "                tp +=1\n",
    "            elif ((prediction[i] == 0) and (batch_y.data[i] == 1)):\n",
    "                fp+=1\n",
    "                #fpl.append(fnames[i])\n",
    "            elif ((prediction[i] == 1) and (batch_y.data[i] == 1)):\n",
    "                tn+=1\n",
    "                #tnl.append(fnames[i])\n",
    "            elif ((prediction[i] == 1) and (batch_y.data[i] == 0)):\n",
    "                fn+=1\n",
    "                #fnl.append(fnames[i])\n",
    "        \n",
    "        \n",
    "\n",
    "      final_acc = 100 * correct / total\n",
    "      prec = tp/(tp+fp)\n",
    "      rec = tp/(tp+fn)\n",
    "      f1 = 2 * ((prec*rec)/(prec+rec))\n",
    "    \n",
    "      self.log_regression.train()\n",
    "      print(f\"Accuracy: {final_acc}, precision: {prec}, recall: {rec}, F1: {f1}\")\n",
    "      return final_acc, prec, rec, f1\n",
    "\n",
    "\n",
    "  def create_data_loaders_from_arrays(self, X_train, y_train, X_test, y_test):\n",
    "    X_train, X_test = self._normalize_dataset(X_train, X_test)\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train).type(torch.long))\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=396, shuffle=False)\n",
    "\n",
    "    test = torch.utils.data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test).type(torch.long))\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=512, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "  def train(self, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    train_loader, test_loader = self.create_data_loaders_from_arrays(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    weight_decay = self._sample_weight_decay()\n",
    "\n",
    "    optimizer = torch.optim.Adam(self.log_regression.parameters(), 3e-4, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for e in range(200):\n",
    "      \n",
    "      for batch_x, batch_y in train_loader:\n",
    "\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = self.log_regression(batch_x)\n",
    "\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "      epoch_acc, prec, rec, f1 = self.eval(test_loader)\n",
    "      \n",
    "      if epoch_acc > best_accuracy:\n",
    "        #print(\"Saving new model with accuracy {}\".format(epoch_acc))\n",
    "        best_accuracy = epoch_acc\n",
    "        best_prec = prec\n",
    "        best_rec = rec\n",
    "        best_f1 = f1\n",
    "        torch.save(self.log_regression.state_dict(), 'log_regression.pth')\n",
    "\n",
    "    print(\"--------------\")\n",
    "    print(\"Done training\")\n",
    "    print(f\"Best accuracy: {best_accuracy}, precision: {best_prec}, recall: {best_rec}, F1: {best_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NE716m7SOkaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaling Normalizer\n",
      "Sampled weight decay: 1e-06\n",
      "--------------\n",
      "Done training\n",
      "Best accuracy: 92.44094488188976, precision: 0.9248554913294798, recall: 0.9238845144356955, F1: 0.9243697478991597\n"
     ]
    }
   ],
   "source": [
    "#Train with 1000 labeled examples\n",
    "log_regressor_evaluator = LogiticRegressionEvaluator(n_features=X_train_feature.shape[1], n_classes=2)\n",
    "\n",
    "log_regressor_evaluator.train(X_train_feature, y_train, X_test_feature, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NE716m7SOkaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaling Normalizer\n",
      "Sampled weight decay: 0.1778279410038923\n",
      "--------------\n",
      "Done training\n",
      "Best accuracy: 92.59186351706036, precision: 0.9185041908446164, recall: 0.9347769028871391, F1: 0.9265691056910569\n"
     ]
    }
   ],
   "source": [
    "#Train with 5000 labeled examples\n",
    "log_regressor_evaluator = LogiticRegressionEvaluator(n_features=X_train_feature.shape[1], n_classes=2)\n",
    "\n",
    "log_regressor_evaluator.train(X_train_feature, y_train, X_test_feature, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NE716m7SOkaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaling Normalizer\n",
      "Sampled weight decay: 100.0\n",
      "--------------\n",
      "Done training\n",
      "Best accuracy: 89.16666666666667, precision: 0.8660615724273274, recall: 0.9266404199475066, F1: 0.8953274583148418\n"
     ]
    }
   ],
   "source": [
    "#Train with 10000 labeled examples\n",
    "log_regressor_evaluator = LogiticRegressionEvaluator(n_features=X_train_feature.shape[1], n_classes=2)\n",
    "\n",
    "log_regressor_evaluator.train(X_train_feature, y_train, X_test_feature, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GC0a14uWRr6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.04461942257218, precision: 0.8877742108079186, recall: 0.870997375328084, F1: 0.8793057763645998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(88.04461942257218, 0.8877742108079186, 0.870997375328084, 0.8793057763645998)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train with 15k labeled examples\n",
    "log_regressor_evaluator = LogiticRegressionEvaluator(n_features=X_train_feature.shape[1], n_classes=2)\n",
    "log_regressor_evaluator.checkpoint_eval('./log_regression.pth', X_test_feature, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "mini-batch-logistic-regression-evaluator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cls",
   "language": "python",
   "name": "cls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
